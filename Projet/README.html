<h1 id="dÃ©tection-de-texte-gÃ©nÃ©rÃ©-par-ia">DÃ©tection de Texte GÃ©nÃ©rÃ© par
IA</h1>
<h3
id="classification-bayÃ©sienne-analyse-factorielle-discriminante">Classification
BayÃ©sienne &amp; Analyse Factorielle Discriminante</h3>
<p><strong>ESIEE Paris â€“ 2025-2026 â€“ E4 AP-4209</strong><br />
<strong>Auteurs :</strong> TORRES Diego, WU Lucas<br />
<strong>Encadrant :</strong> Badr TAJINI</p>
<hr />
<h2 id="table-des-matiÃ¨res">Table des matiÃ¨res</h2>
<ol style="list-style-type: decimal">
<li><a href="#id_-vue-densemble">Vue dâ€™ensemble</a></li>
<li><a href="#id_-dataset">Dataset</a></li>
<li><a href="#id_-pipeline-compl%C3%A8te">Pipeline complÃ¨te</a></li>
<li><a href="#id_-structure-du-projet">Structure du projet</a></li>
<li><a href="#id_-installation-et-d%C3%A9pendances">Installation et
dÃ©pendances</a></li>
<li><a href="#id_-utilisation">Utilisation</a></li>
<li><a href="#id_-r%C3%A9sultats">RÃ©sultats</a></li>
<li><a href="#id_-m%C3%A9thodes-et-fondements-th%C3%A9oriques">MÃ©thodes
et fondements thÃ©oriques</a></li>
<li><a href="#id_-limites-et-travaux-futurs">Limites et travaux
futurs</a></li>
<li><a href="#id_-r%C3%A9f%C3%A9rences">RÃ©fÃ©rences</a></li>
</ol>
<hr />
<h2 id="vue-densemble">Vue dâ€™ensemble</h2>
<p>Ce projet implÃ©mente un <strong>systÃ¨me complet de dÃ©tection de texte
gÃ©nÃ©rÃ© par IA</strong>, opposant des textes humains (label
<code>0</code>) Ã  des textes produits par des LLMs (label
<code>1</code>). Il sâ€™inscrit dans le challenge Kaggle <a
href="https://www.kaggle.com/competitions/llm-detect-ai-generated-text">LLM
- Detect AI Generated Text</a>.</p>
<p>Lâ€™approche repose sur une pipeline Ã  cinq Ã©tapes :</p>
<pre><code>Texte brut
    â†“
Features stylomÃ©triques (17 mÃ©triques linguistiques)
    +
TF-IDF bigrammes (332 486 termes)
    â†“
SVD tronquÃ©e â€” k = 150 dimensions latentes
    â†“
AFD (Analyse Factorielle Discriminante)  â†’  1 axe LD1 (Cohen&#39;s d = 5.49)
    â†“
Classification BayÃ©sienne MCMC (NUTS/HMC via rstanarm)
    â†“
ProbabilitÃ©s calibrÃ©es P(texte IA)</code></pre>
<hr />
<h2 id="dataset">Dataset</h2>
<h3 id="source-principale-llm-detect-kaggle">Source principale â€”
LLM-Detect (Kaggle)</h3>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr>
<th>Fichier</th>
<th>Description</th>
<th>Lignes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>train_essays.csv</code></td>
<td>Essais annotÃ©s (Kaggle officiel)</td>
<td>1 378</td>
</tr>
<tr>
<td><code>test_essays.csv</code></td>
<td>Essais de test (soumission Kaggle)</td>
<td>3 (jeu exemple)</td>
</tr>
<tr>
<td><code>sample_submission.csv</code></td>
<td>Format de soumission attendu</td>
<td>â€”</td>
</tr>
</tbody>
</table>
<h3 id="source-complÃ©mentaire-drcat">Source complÃ©mentaire â€” DRCAT</h3>
<table>
<thead>
<tr>
<th>Fichier</th>
<th>Description</th>
<th>Lignes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>train_drcat_01.csv</code></td>
<td>Textes humains &amp; IA (fold 1)</td>
<td>~40 000</td>
</tr>
<tr>
<td><code>train_drcat_02.csv</code></td>
<td>Textes humains &amp; IA (fold 2)</td>
<td>~40 000</td>
</tr>
<tr>
<td><code>train_drcat_03.csv</code></td>
<td>Textes humains &amp; IA (fold 3)</td>
<td>~40 000</td>
</tr>
<tr>
<td><code>train_drcat_04.csv</code></td>
<td>Textes humains &amp; IA (fold 4)</td>
<td>~40 000</td>
</tr>
</tbody>
</table>
<p><strong>Dataset consolidÃ© aprÃ¨s fusion et nettoyage :</strong></p>
<table>
<thead>
<tr>
<th>Classe</th>
<th>Effectif</th>
<th>Proportion</th>
</tr>
</thead>
<tbody>
<tr>
<td>Humain (0)</td>
<td>116 747</td>
<td>72,6 %</td>
</tr>
<tr>
<td>IA (1)</td>
<td>44 087</td>
<td>27,4 %</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>160 834</strong></td>
<td>â€”</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Structure CSV attendue :</strong> colonnes <code>text</code>
et <code>generated</code> (0 ou 1). Les fichiers DRCAT peuvent avoir une
colonne <code>label</code> qui est automatiquement renommÃ©e.</p>
</blockquote>
<h3 id="placement-des-fichiers">Placement des fichiers</h3>
<pre><code>projet_final/
â””â”€â”€ data/
    â”œâ”€â”€ llm-detect-ai-generated-text/
    â”‚   â””â”€â”€ llm-detect-ai-generated-text/
    â”‚       â”œâ”€â”€ train_essays.csv
    â”‚       â”œâ”€â”€ test_essays.csv
    â”‚       â””â”€â”€ sample_submission.csv
    â”œâ”€â”€ train_drcat_01.csv
    â”œâ”€â”€ train_drcat_02.csv
    â”œâ”€â”€ train_drcat_03.csv
    â””â”€â”€ train_drcat_04.csv</code></pre>
<hr />
<h2 id="pipeline-complÃ¨te">Pipeline complÃ¨te</h2>
<h3 id="Ã©tape-1-prÃ©traitement-nettoyage">Ã‰tape 1 â€” PrÃ©traitement &amp;
Nettoyage</h3>
<ul>
<li>Fusion des sources Kaggle et DRCAT</li>
<li>Nettoyage du texte (retrait des doublons, suppression des entrÃ©es
vides, filtre <code>nchar &gt; 30</code>)</li>
<li>DÃ©tection et correction des <code>NA</code> via
<code>safe_texts()</code> (critique pour <code>itoken</code>)</li>
</ul>
<h3 id="Ã©tape-2-extraction-de-features-stylomÃ©triques">Ã‰tape 2 â€”
Extraction de features stylomÃ©triques</h3>
<p>17 mÃ©triques linguistiques extraites en parallÃ¨le (21 cÅ“urs) avec
mise en cache <code>.rds</code> :</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>char_count</code></td>
<td>Nombre total de caractÃ¨res</td>
</tr>
<tr>
<td><code>word_count</code></td>
<td>Nombre total de mots</td>
</tr>
<tr>
<td><code>sent_count</code></td>
<td>Nombre de phrases</td>
</tr>
<tr>
<td><code>avg_sent_len</code></td>
<td>Longueur moyenne des phrases (mots)</td>
</tr>
<tr>
<td><code>sent_len_sd</code></td>
<td><strong>VariabilitÃ©</strong> des longueurs de phrases â† discriminant
fort</td>
</tr>
<tr>
<td><code>avg_word_len</code></td>
<td>Longueur moyenne des mots</td>
</tr>
<tr>
<td><code>ttr</code></td>
<td><strong>Type-Token Ratio</strong> â€” richesse du vocabulaire</td>
</tr>
<tr>
<td><code>hapax_ratio</code></td>
<td>Mots nâ€™apparaissant quâ€™une seule fois</td>
</tr>
<tr>
<td><code>flesch</code></td>
<td>Score de lisibilitÃ© de Flesch (approchÃ©)</td>
</tr>
<tr>
<td><code>lex_entropy</code></td>
<td>Entropie lexicale</td>
</tr>
<tr>
<td><code>punct_rate</code></td>
<td>Taux de ponctuation</td>
</tr>
<tr>
<td><code>comma_rate</code></td>
<td>Taux de virgules</td>
</tr>
<tr>
<td><code>upper_rate</code></td>
<td>Taux de majuscules</td>
</tr>
<tr>
<td><code>discourse_markers</code></td>
<td>Connecteurs formels typiques des textes IA</td>
</tr>
<tr>
<td><code>ai_phrases</code></td>
<td>Expressions gÃ©nÃ©riques dÃ©tectÃ©es dans les textes IA</td>
</tr>
<tr>
<td><code>long_word_rate</code></td>
<td>Taux de mots longs (&gt; 6 caractÃ¨res)</td>
</tr>
<tr>
<td><code>char_bigram_entropy</code></td>
<td><strong>Proxy de perplexitÃ©</strong> via entropie des bigrammes de
caractÃ¨res</td>
</tr>
</tbody>
</table>
<h3 id="Ã©tape-3-vectorisation-tf-idf-svd">Ã‰tape 3 â€” Vectorisation TF-IDF
+ SVD</h3>
<pre><code>TF-IDF bigrammes :  128 668 documents Ã— 332 486 termes
         â†“ SVD tronquÃ©e (irlba, k = 150)
Espace dense :      128 668 documents Ã— 150 dimensions</code></pre>
<ul>
<li>Vocabulaire unigrammes + bigrammes
(<code>ngram = c(1L, 2L)</code>)</li>
<li>Filtres : <code>term_count_min = 5</code>,
<code>doc_proportion_max = 0.45</code></li>
<li>SVD : 54,7 % de la variance capturÃ©e dÃ¨s k = 20 ; 100 % pour k = 150
(variance relative aux 150 valeurs singuliÃ¨res calculÃ©es)</li>
</ul>
<h3 id="Ã©tape-4-analyse-factorielle-discriminante-afd">Ã‰tape 4 â€” Analyse
Factorielle Discriminante (AFD)</h3>
<p>Lâ€™AFD cherche la projection <strong>w</strong> maximisant le critÃ¨re
de Fisher :</p>
<p><span class="math display">$$J(\mathbf{w}) = \frac{\mathbf{w}^\top
S_B \,\mathbf{w}}{\mathbf{w}^\top S_W \,\mathbf{w}}$$</span></p>
<p>avec <span class="math inline"><em>S</em><sub><em>B</em></sub></span>
= dispersion inter-classes, <span
class="math inline"><em>S</em><sub><em>W</em></sub></span> = dispersion
intra-classes.</p>
<p>Pour K = 2 classes, il existe <strong>un unique axe discriminant
LD1</strong>.</p>
<p><strong>RÃ©sultat obtenu :</strong></p>
<table>
<thead>
<tr>
<th>MÃ©trique</th>
<th>Valeur</th>
<th>InterprÃ©tation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Moyenne LD1 Humain</td>
<td>âˆ’1.715</td>
<td>Projection nÃ©gative</td>
</tr>
<tr>
<td>Moyenne LD1 IA</td>
<td>+4.560</td>
<td>Projection positive</td>
</tr>
<tr>
<td>Cohenâ€™s d</td>
<td><strong>5.49</strong></td>
<td>Effet <strong>Grand</strong> (d &gt; 0.8)</td>
</tr>
<tr>
<td>Score silhouette</td>
<td><strong>0.806</strong></td>
<td>Bonne sÃ©paration</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Justification de lâ€™AFD linÃ©aire :</strong> La forte
sÃ©paration linÃ©aire (Cohenâ€™s d = 5.49) observÃ©e sur LD1 valide le choix
de lâ€™AFD classique. Une Kernel Discriminant Analysis (KDA) apporterait
une complexitÃ© supplÃ©mentaire sans gain attendu dans ce contexte.</p>
</blockquote>
<h3 id="Ã©tape-5-classification-bayÃ©sienne-mcmc">Ã‰tape 5 â€” Classification
BayÃ©sienne MCMC</h3>
<p>ModÃ¨le logistique bayÃ©sien :</p>
<p><span
class="math display">logit(<em>p</em><sub><em>i</em></sub>)â€„=â€„<em>Î±</em>â€…+â€…<em>Î²</em>â€…â‹…â€…LD1<sub><em>i</em></sub></span></p>
<table>
<thead>
<tr>
<th>ParamÃ¨tre</th>
<th>Prior</th>
<th>Justification</th>
</tr>
</thead>
<tbody>
<tr>
<td>Î²</td>
<td>N(0, 2.5) autoscalÃ©</td>
<td>RÃ©gularisation Ridge bayÃ©sienne</td>
</tr>
<tr>
<td>Î±</td>
<td>N(0, 5)</td>
<td>Prior diffus sur lâ€™intercept</td>
</tr>
</tbody>
</table>
<p><strong>Algorithme :</strong> NUTS (No-U-Turn Sampler), variante
adaptative de HMC â€” 4 chaÃ®nes Ã— 2000 itÃ©rations (warmup = 1000).</p>
<p><strong>Diagnostics MCMC :</strong> - RÌ‚ max = <strong>1.0006</strong>
(seuil &lt; 1.01 âœ…) - n_eff â‰ˆ 1 600â€“3 200 (trÃ¨s satisfaisant) -
Convergence confirmÃ©e sur toutes les chaÃ®nes</p>
<p><strong>Estimations a posteriori (IC 95 %) :</strong></p>
<table>
<thead>
<tr>
<th>ParamÃ¨tre</th>
<th>Moyenne</th>
<th>SD</th>
<th>2.5%</th>
<th>97.5%</th>
</tr>
</thead>
<tbody>
<tr>
<td>Î± (Intercept)</td>
<td>âˆ’3.064</td>
<td>0.048</td>
<td>âˆ’3.161</td>
<td>âˆ’2.975</td>
</tr>
<tr>
<td>Î² (LD1)</td>
<td>+3.033</td>
<td>0.044</td>
<td>+2.951</td>
<td>+3.123</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="structure-du-projet">Structure du projet</h2>
<pre><code>projet_final/
â”‚
â”œâ”€â”€ data/                              # DonnÃ©es brutes (non versionnÃ©es)
â”‚   â”œâ”€â”€ llm-detect-ai-generated-text/
â”‚   â””â”€â”€ train_drcat_0[1-4].csv
â”‚
â”œâ”€â”€ projet_final.Rmd                   # Document principal (code + rapport)
â”œâ”€â”€ projet_final.html                  # Rapport rendu (sortie RMarkdown)
â”œâ”€â”€ submission.csv                     # PrÃ©dictions pour soumission Kaggle
â”‚
â”œâ”€â”€ cache_stylo_all.rds               # Cache features stylomÃ©triques (train)
â”œâ”€â”€ cache_stylo_cv.rds                # Cache features stylomÃ©triques (CV)
â”œâ”€â”€ cache_stylo_test.rds              # Cache features stylomÃ©triques (test)
â”‚
â””â”€â”€ README.md                          # Ce fichier</code></pre>
<hr />
<h2 id="installation-et-dÃ©pendances">ğŸ”§ Installation et dÃ©pendances</h2>
<h3 id="prÃ©requis">PrÃ©requis</h3>
<ul>
<li><strong>R</strong> â‰¥ 4.3.0</li>
<li><strong>RStudio</strong> (recommandÃ©) ou tout Ã©diteur compatible
RMarkdown</li>
<li>MÃ©moire RAM recommandÃ©e : <strong>â‰¥ 16 Go</strong> (dataset de 160
000 textes)</li>
<li>CPU multi-cÅ“urs recommandÃ© (extraction stylomÃ©trique parallÃ©lisÃ©e
sur 21 cÅ“urs)</li>
</ul>
<h3 id="installation-des-packages">Installation des packages</h3>
<p>Coller dans la console R <strong>avant</strong> de lancer le render
:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  <span class="co"># Manipulation de donnÃ©es</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  <span class="st">&quot;readr&quot;</span>, <span class="st">&quot;dplyr&quot;</span>, <span class="st">&quot;stringr&quot;</span>, <span class="st">&quot;tidyr&quot;</span>, <span class="st">&quot;tibble&quot;</span>,</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>  </span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>  <span class="co"># Visualisation</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>  <span class="st">&quot;ggplot2&quot;</span>, <span class="st">&quot;gridExtra&quot;</span>, <span class="st">&quot;scales&quot;</span>,</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>  </span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>  <span class="co"># NLP / Vectorisation</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>  <span class="st">&quot;text2vec&quot;</span>, <span class="st">&quot;irlba&quot;</span>,</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>  </span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>  <span class="co"># Topic modeling</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>  <span class="st">&quot;topicmodels&quot;</span>, <span class="st">&quot;slam&quot;</span>, <span class="st">&quot;tidytext&quot;</span>,</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>  </span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>  <span class="co"># ModÃ©lisation</span></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>  <span class="st">&quot;MASS&quot;</span>, <span class="st">&quot;caret&quot;</span>,</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>  </span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>  <span class="co"># Classification BayÃ©sienne MCMC</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>  <span class="st">&quot;rstanarm&quot;</span>,        <span class="co"># Stan â€” NUTS/HMC (backend principal)</span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>  <span class="st">&quot;arm&quot;</span>,             <span class="co"># bayesglm â€” fallback si Stan non disponible</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>  <span class="st">&quot;bayesplot&quot;</span>,       <span class="co"># Visualisation des posteriors</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>  </span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>  <span class="co"># Ã‰valuation</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>  <span class="st">&quot;pROC&quot;</span>, <span class="st">&quot;cluster&quot;</span></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>))</span></code></pre></div>
<blockquote>
<p><strong>Important :</strong> <code>rstanarm</code> requiert
lâ€™installation de <strong>Stan</strong>. Sur Windows, il peut Ãªtre
nÃ©cessaire dâ€™installer <a
href="https://cran.r-project.org/bin/windows/Rtools/">Rtools</a> au
prÃ©alable.<br />
Si <code>rstanarm</code> nâ€™est pas disponible, le code bascule
automatiquement sur <code>arm::bayesglm</code> (estimateur MAP â€”
fallback sans MCMC complet).</p>
</blockquote>
<hr />
<h2 id="utilisation">Utilisation</h2>
<h3 id="lancement-du-rapport-complet">Lancement du rapport complet</h3>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>rmarkdown<span class="sc">::</span><span class="fu">render</span>(<span class="st">&quot;projet_final.Rmd&quot;</span>)</span></code></pre></div>
<p>Ou depuis le terminal :</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode powershell"><code class="sourceCode powershell"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="op">&amp;</span> <span class="st">&quot;C:\Program Files\R\R-4.5.2\bin\Rscript.exe&quot;</span> <span class="op">-</span>e <span class="st">&quot;rmarkdown::render(&#39;projet_final.Rmd&#39;)&quot;</span></span></code></pre></div>
<h3 id="paramÃ¨tres-clÃ©s-modifiables-dans-le-rmd">ParamÃ¨tres clÃ©s
modifiables dans le Rmd</h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>USE_RSTANARM <span class="ot">&lt;-</span> <span class="cn">TRUE</span>     <span class="co"># FALSE = utilise arm::bayesglm (plus rapide, moins complet)</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>K_SVD        <span class="ot">&lt;-</span> <span class="dv">150</span>      <span class="co"># Nombre de dimensions SVD (compromis vitesse/prÃ©cision)</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>K_TOPICS     <span class="ot">&lt;-</span> <span class="dv">6</span>        <span class="co"># Nombre de topics LDA thÃ©matique</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>MAX_CV_N     <span class="ot">&lt;-</span> <span class="dv">6000</span>     <span class="co"># Taille du sous-Ã©chantillon pour la validation croisÃ©e</span></span></code></pre></div>
<h3 id="cache-stylomÃ©trique">Cache stylomÃ©trique</h3>
<p>Lâ€™extraction stylomÃ©trique est <strong>mise en cache
automatiquement</strong> (fichiers <code>.rds</code>). Pour forcer un
recalcul, supprimez les fichiers <code>cache_stylo_*.rds</code> avant de
relancer.</p>
<hr />
<h2 id="rÃ©sultats">RÃ©sultats</h2>
<h3 id="mÃ©triques-sur-lensemble-de-validation-8020-stratifiÃ©">MÃ©triques
sur lâ€™ensemble de validation (80/20 stratifiÃ©)</h3>
<table>
<thead>
<tr>
<th>MÃ©trique</th>
<th>Valeur</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Accuracy</strong></td>
<td><strong>99.34 %</strong></td>
</tr>
<tr>
<td>Precision</td>
<td>99.14 %</td>
</tr>
<tr>
<td>Rappel (SensibilitÃ©)</td>
<td>98.49 %</td>
</tr>
<tr>
<td>SpÃ©cificitÃ©</td>
<td>99.67 %</td>
</tr>
<tr>
<td><strong>F1-score</strong></td>
<td><strong>98.81 %</strong></td>
</tr>
<tr>
<td><strong>AUC-ROC</strong></td>
<td><strong>0.9993</strong></td>
</tr>
<tr>
<td><strong>Score de Brier</strong></td>
<td><strong>0.0053</strong></td>
</tr>
<tr>
<td>Kappa de Cohen</td>
<td>0.9836</td>
</tr>
<tr>
<td>Seuil optimal F1</td>
<td>0.43</td>
</tr>
</tbody>
</table>
<h3 id="validation-croisÃ©e-5-fold-stratifiÃ©e">Validation croisÃ©e 5-fold
stratifiÃ©e</h3>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr>
<th>Fold</th>
<th>AUC</th>
<th>Brier</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.9979</td>
<td>0.0119</td>
<td>98.67 %</td>
</tr>
<tr>
<td>2</td>
<td>0.9968</td>
<td>0.0091</td>
<td>99.08 %</td>
</tr>
<tr>
<td>3</td>
<td>0.9991</td>
<td>0.0080</td>
<td>99.00 %</td>
</tr>
<tr>
<td>4</td>
<td>0.9990</td>
<td>0.0111</td>
<td>98.25 %</td>
</tr>
<tr>
<td>5</td>
<td>0.9973</td>
<td>0.0155</td>
<td>98.08 %</td>
</tr>
<tr>
<td><strong>Moyenne</strong></td>
<td><strong>0.9980 Â± 0.0010</strong></td>
<td><strong>0.0111 Â± 0.0029</strong></td>
<td><strong>98.62 % Â± 0.44 %</strong></td>
</tr>
</tbody>
</table>
<h3 id="sÃ©paration-afd">SÃ©paration AFD</h3>
<pre><code>Classe Humain : LD1 = âˆ’1.715  (Ïƒ = 0.792)
Classe IA     : LD1 = +4.560  (Ïƒ = 1.410)
Cohen&#39;s d     : 5.49  â†’  Effet Grand
Score silhouette : 0.806  â†’  Bonne sÃ©paration</code></pre>
<h3 id="topics-lda-k-6-gibbs-sous-Ã©chantillon-5-000-docs">Topics LDA (k
= 6, Gibbs, sous-Ã©chantillon 5 000 docs)</h3>
<table>
<thead>
<tr>
<th>Topic</th>
<th>ThÃ¨me identifiÃ©</th>
<th>Mots clÃ©s</th>
</tr>
</thead>
<tbody>
<tr>
<td>Topic 1</td>
<td>Ã‰ducation</td>
<td>students, school, learning, classes</td>
</tr>
<tr>
<td>Topic 2</td>
<td>Transport / Vote</td>
<td>car, driving, Electoral, vote</td>
</tr>
<tr>
<td>Topic 3</td>
<td>Conseil / Aide</td>
<td>how, could, know, someone, better</td>
</tr>
<tr>
<td>Topic 4</td>
<td>Opinion personnelle</td>
<td>we, think, want, good, my</td>
</tr>
<tr>
<td>Topic 5</td>
<td>Sciences / Espace</td>
<td>Venus, face, author, is_a</td>
</tr>
<tr>
<td>Topic 6</td>
<td>Argumentation</td>
<td>may, important, can_be, lead</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="mÃ©thodes-et-fondements-thÃ©oriques">MÃ©thodes et fondements
thÃ©oriques</h2>
<h3 id="thÃ©orÃ¨me-de-bayes">ThÃ©orÃ¨me de Bayes</h3>
<p><span
class="math display"><em>P</em>(<strong>Î¸</strong>â€…âˆ£â€…ğ’Ÿ)â€„âˆâ€„<em>P</em>(ğ’Ÿâ€…âˆ£â€…<strong>Î¸</strong>)â€…â‹…â€…<em>P</em>(<strong>Î¸</strong>)</span></p>
<p>Le modÃ¨le logistique bayÃ©sien prÃ©dit la probabilitÃ© quâ€™un texte soit
gÃ©nÃ©rÃ© par IA. Lâ€™infÃ©rence complÃ¨te via MCMC fournit des
<strong>distributions a posteriori</strong> sur les paramÃ¨tres, pas
simplement des estimations ponctuelles.</p>
<h3 id="critÃ¨re-de-fisher-afd">CritÃ¨re de Fisher (AFD)</h3>
<p><span class="math display">$$J(\mathbf{w}) = \frac{\mathbf{w}^\top
S_B \,\mathbf{w}}{\mathbf{w}^\top S_W \,\mathbf{w}}$$</span></p>
<p>RÃ©solu comme un problÃ¨me aux valeurs propres gÃ©nÃ©ralisÃ© <span
class="math inline"><em>S</em><sub><em>W</em></sub><sup>âˆ’1</sup><em>S</em><sub><em>B</em></sub>â€†<strong>w</strong>â€„=â€„<em>Î»</em>â€†<strong>w</strong></span>.</p>
<h3 id="pourquoi-svd-avant-afd">Pourquoi SVD avant AFD ?</h3>
<p>La matrice TF-IDF brute est <strong>creuse</strong> et de dimension
&gt;&gt; 10 000 colonnes, rendant lâ€™inversion de <span
class="math inline"><em>S</em><sub><em>W</em></sub></span> numÃ©riquement
instable. La SVD vers k = 150 dimensions denses (1) stabilise le calcul,
(2) Ã©limine le bruit lexical, (3) approche la normalitÃ© multivariÃ©e
requise par lâ€™AFD.</p>
<h3 id="no-u-turn-sampler-nuts">No-U-Turn Sampler (NUTS)</h3>
<p>Variante adaptative de HMC (Hamiltonian Monte Carlo) qui explore
lâ€™espace a posteriori bien plus efficacement que Metropolis-Hastings
classique, Ã©vitant les random walks et le rÃ©glage manuel du pas
dâ€™intÃ©gration.</p>
<hr />
<h2 id="limites-et-travaux-futurs">Limites et travaux futurs</h2>
<h3 id="limites-identifiÃ©es">Limites identifiÃ©es</h3>
<ul>
<li><strong>Performances Ã©levÃ©es â€” interprÃ©tation prudente :</strong>
Les textes IA du dataset LLM-Detect prÃ©sentent des patterns stylistiques
trÃ¨s distincts. Une validation par groupes (par source ou prompt)
permettrait de dÃ©tecter un Ã©ventuel leakage contextuel liÃ© Ã  la
structure du dataset.</li>
<li><strong>Jeu de test rÃ©duit :</strong> Le fichier
<code>test_essays.csv</code> officiel Kaggle ne contient que 3 exemples
â€” la pipeline de soumission est fonctionnelle quelle que soit la taille
rÃ©elle du jeu de test.</li>
<li><strong>Absence de perplexitÃ© rÃ©elle :</strong> La feature
<code>char_bigram_entropy</code> est un proxy ; une vraie perplexitÃ©
GPT-2 (via <code>reticulate</code>) serait plus discriminante.</li>
</ul>
<h3 id="travaux-futurs">Travaux futurs</h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr>
<th>Piste</th>
<th>BÃ©nÃ©fice attendu</th>
</tr>
</thead>
<tbody>
<tr>
<td>PerplexitÃ© GPT-2 via <code>reticulate</code></td>
<td>Feature trÃ¨s discriminante documentÃ©e dans la littÃ©rature</td>
</tr>
<tr>
<td>BERT / sentence embeddings</td>
<td>Remplacement du TF-IDF par des reprÃ©sentations contextuelles</td>
</tr>
<tr>
<td>Kernel Discriminant Analysis (KDA)</td>
<td>Capturer dâ€™Ã©ventuelles non-linÃ©aritÃ©s inter-classes</td>
</tr>
<tr>
<td>InfÃ©rence variationnelle (<code>algorithm = "meanfield"</code>)</td>
<td>Passage Ã  lâ€™Ã©chelle sur trÃ¨s grands corpus</td>
</tr>
<tr>
<td>LOO-CV / WAIC via <code>rstanarm::loo()</code></td>
<td>Comparaison formelle de modÃ¨les bayÃ©siens</td>
</tr>
<tr>
<td>Validation par groupe (leave-one-prompt-out)</td>
<td>Ã‰valuation de la robustesse hors-distribution</td>
</tr>
<tr>
<td>Test de Boxâ€™s M</td>
<td>VÃ©rification de lâ€™homoscÃ©dasticitÃ© (hypothÃ¨se AFD)</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="rÃ©fÃ©rences">RÃ©fÃ©rences</h2>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr>
<th>RÃ©fÃ©rence</th>
<th>Lien / Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dataset LLM-Detect</strong></td>
<td><a
href="https://www.kaggle.com/competitions/llm-detect-ai-generated-text">Kaggle
â€” LLM Detect AI Generated Text</a></td>
</tr>
<tr>
<td><strong>Dataset DRCAT</strong></td>
<td>ComplÃ©ment dâ€™entraÃ®nement avec essais humains et IA</td>
</tr>
<tr>
<td><strong>rstanarm</strong></td>
<td>Goodrich et al.Â (2023). <em>rstanarm: Bayesian applied regression
modeling via Stan.</em> CRAN</td>
</tr>
<tr>
<td><strong>Stan / NUTS</strong></td>
<td>Carpenter et al.Â (2017). <em>Stan: A probabilistic programming
language.</em> JOSS</td>
</tr>
<tr>
<td><strong>Fisher LDA</strong></td>
<td>Fisher, R.A. (1936). <em>The use of multiple measurements in
taxonomic problems.</em> Annals of Eugenics</td>
</tr>
<tr>
<td><strong>SVD / LSA</strong></td>
<td>Deerwester et al.Â (1990). <em>Indexing by Latent Semantic
Analysis.</em> JASIS</td>
</tr>
<tr>
<td><strong>irlba</strong></td>
<td>Baglama &amp; Reichel (2005). <em>Augmented implicitly restarted
Lanczos bidiagonalization methods.</em> SIAM</td>
</tr>
<tr>
<td><strong>text2vec</strong></td>
<td>Selivanov, D. (2023). <em>text2vec: Modern Text Mining Framework for
R.</em> CRAN</td>
</tr>
<tr>
<td><strong>StylomÃ©trie</strong></td>
<td>Stamatatos, E. (2009). <em>A survey of modern authorship attribution
methods.</em> JASIS&amp;T</td>
</tr>
<tr>
<td><strong>Score de Brier</strong></td>
<td>Brier, G.W. (1950). <em>Verification of forecasts expressed in terms
of probability.</em> Monthly Weather Review</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="auteurs">Auteurs</h2>
<table>
<thead>
<tr>
<th>Nom</th>
<th>Email</th>
<th>Ã‰tablissement</th>
</tr>
</thead>
<tbody>
<tr>
<td>TORRES Diego</td>
<td>â€”</td>
<td>ESIEE Paris</td>
</tr>
<tr>
<td>WU Lucas</td>
<td>â€”</td>
<td>ESIEE Paris</td>
</tr>
</tbody>
</table>
<p><strong>Encadrant :</strong> Badr TAJINI â€” ESIEE Paris<br />
<strong>Cours :</strong> AP-4209 â€” E4 â€” 2025-2026<br />
<strong>Rapport gÃ©nÃ©rÃ© le :</strong> 19/02/2026</p>
<hr />
<p><em>README rÃ©digÃ© en correspondance avec le rapport
<code>projet_final.html</code> et le sujet
<code>final_project.pdf</code>.</em></p>
